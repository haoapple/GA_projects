{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Using Reddit's API for Predicting Comments\n",
    "### Author: Kihoon Sohn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- Notebook 1 - Data Fetching: `json` webscrap and unpack to dataframe\n",
    "- Notebook 2 - Data Cleansing: exploratory data analysis and feature engineering\n",
    "- **Notebook 3 - Data Modeling(current)**: build a predictive model\n",
    "\n",
    "**Disclaimer**: Due to the file size restriction in GitHub, `/dataset/` folder and other large files were ignored by `.gitignore`. Therefore the notebook might not reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "# set random seed\n",
    "import numpy as np\n",
    "np.random.seed(538)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a: Read CSV for NLP (51K hotposts from reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV\n",
    "df = pd.read_csv('./dataset/master_06-02-2018(hotposts).csv')\n",
    "df.drop(columns='Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'subreddit', 'num_comments', 'created_utc',\n",
      "       'fetched time', 'age', 'comments'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>fetched time</th>\n",
       "      <th>age</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8m1wov</td>\n",
       "      <td>It happens in anime, it happens in life</td>\n",
       "      <td>combinedgifs</td>\n",
       "      <td>407</td>\n",
       "      <td>2018-05-25 13:52:24</td>\n",
       "      <td>2018-05-25 20:10:12</td>\n",
       "      <td>377.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                    title     subreddit  \\\n",
       "0  8m1wov  It happens in anime, it happens in life  combinedgifs   \n",
       "\n",
       "   num_comments          created_utc         fetched time    age  comments  \n",
       "0           407  2018-05-25 13:52:24  2018-05-25 20:10:12  377.0         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.columns)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b: Build model and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree-based Model 1 - Feature: `subreddit` with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X and y\n",
    "y = df['comments']\n",
    "X = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy all the subreddits\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7618310767246937"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7886533838118661"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compare score between train / test set.  \n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7636720369193357\n",
      "{'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'n_estimators' : [10, 15, 20]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid=rf_params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7623468729851709"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 result\n",
    "- shown some improvement compared to baseline accuracy.\n",
    "- test set score is better than train set score.\n",
    "- Gridsearch changes the score slightly increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree-based Model 2 - Features: `subreddit`, `is_cat`, `is_funny` with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X and y\n",
    "\n",
    "y = df['comments']\n",
    "X = df[['subreddit', 'title']].copy(deep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's boolean for cat/funny is in the title\n",
    "X['is_cat'] = X['title'].map(lambda x: 1 if 'cat' in x else 0)\n",
    "X['is_funny'] = X['title'].map(lambda x: 1 if 'funny' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'title' and dummy all of it.\n",
    "X.drop('title', axis=1, inplace=True)\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7607350096711799"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7909193909414983"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compare score between train / test set.  \n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7635614999861828\n",
      "{'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'n_estimators' : [10, 15, 20]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid=rf_params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622179239200516"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 result\n",
    "- compared to the Model 1, `is_cat` and `is_funny` doesn't affect much in the score changes. Therefore, it didn't show any meaningful improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree-based Model 3 - Feature: `title`, `subreddit`, `age` with TfidfVectorizer & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X and y\n",
    "\n",
    "y = df['comments']\n",
    "X = df[['subreddit', 'title', 'age']].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy all the subreddits\n",
    "X = pd.get_dummies(X, columns=['subreddit'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train_matrix = tvec.fit_transform(X_train['title'])\n",
    "X_test_matrix = tvec.transform(X_test['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train_matrix.todense(),\n",
    "                         columns=tvec.get_feature_names(),\n",
    "                         index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = pd.DataFrame(X_test_matrix.todense(),\n",
    "                        columns=tvec.get_feature_names(),\n",
    "                        index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.index.all() == X_train_df.index.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_all = pd.concat([X_train_df, X_train.drop('title', axis=1)], axis=1)\n",
    "X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000ft</th>\n",
       "      <th>000k</th>\n",
       "      <th>000km</th>\n",
       "      <th>000kr</th>\n",
       "      <th>000lb</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>00100000</th>\n",
       "      <th>...</th>\n",
       "      <th>subreddit_yourmomshousepodcast</th>\n",
       "      <th>subreddit_youseeingthisshit</th>\n",
       "      <th>subreddit_youtube</th>\n",
       "      <th>subreddit_youtubehaiku</th>\n",
       "      <th>subreddit_yuruyuri</th>\n",
       "      <th>subreddit_yvonnestrahovski</th>\n",
       "      <th>subreddit_zelda</th>\n",
       "      <th>subreddit_zen</th>\n",
       "      <th>subreddit_zerocarb</th>\n",
       "      <th>subreddit_zuckmemes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34130</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14459</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38790 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  000ft  000k  000km  000kr  000lb  000th  001  00100000  \\\n",
       "24100  0.0  0.0    0.0   0.0    0.0    0.0    0.0    0.0  0.0       0.0   \n",
       "34130  0.0  0.0    0.0   0.0    0.0    0.0    0.0    0.0  0.0       0.0   \n",
       "36106  0.0  0.0    0.0   0.0    0.0    0.0    0.0    0.0  0.0       0.0   \n",
       "35994  0.0  0.0    0.0   0.0    0.0    0.0    0.0    0.0  0.0       0.0   \n",
       "14459  0.0  0.0    0.0   0.0    0.0    0.0    0.0    0.0  0.0       0.0   \n",
       "\n",
       "              ...           subreddit_yourmomshousepodcast  \\\n",
       "24100         ...                                        0   \n",
       "34130         ...                                        0   \n",
       "36106         ...                                        0   \n",
       "35994         ...                                        0   \n",
       "14459         ...                                        0   \n",
       "\n",
       "       subreddit_youseeingthisshit  subreddit_youtube  subreddit_youtubehaiku  \\\n",
       "24100                            0                  0                       0   \n",
       "34130                            0                  0                       0   \n",
       "36106                            0                  0                       0   \n",
       "35994                            0                  0                       0   \n",
       "14459                            0                  0                       0   \n",
       "\n",
       "       subreddit_yuruyuri  subreddit_yvonnestrahovski  subreddit_zelda  \\\n",
       "24100                   0                           0                0   \n",
       "34130                   0                           0                0   \n",
       "36106                   0                           0                0   \n",
       "35994                   0                           0                0   \n",
       "14459                   0                           0                0   \n",
       "\n",
       "       subreddit_zen  subreddit_zerocarb  subreddit_zuckmemes  \n",
       "24100              0                   0                    0  \n",
       "34130              0                   0                    0  \n",
       "36106              0                   0                    0  \n",
       "35994              0                   0                    0  \n",
       "14459              0                   0                    0  \n",
       "\n",
       "[5 rows x 38790 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8034816247582205"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_all, y_train)\n",
    "rf.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9793295935004284"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to compare score between train / test set.  \n",
    "rf.score(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8058142426838367\n",
      "{'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'n_estimators' : [10, 15, 20]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid=rf_params)\n",
    "gs.fit(X_train_all, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8104448742746615"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 result\n",
    "- train set score and test set score difference is quite huge. It can be said that the model is overfitted.\n",
    "- However, among the previous two models, it shows the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree-based Model 4 - Feature: `title` with CountVectorizer, TfidfVectorizer, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X and y\n",
    "\n",
    "y = df['comments']\n",
    "X = df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the pipeline\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english', max_features=500)\n",
    "tvec = TfidfVectorizer(stop_words='english', max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 64)\t1\n",
      "  (0, 94)\t1\n",
      "  (0, 138)\t1\n",
      "  (0, 495)\t1\n",
      "  (1, 411)\t1\n",
      "  (2, 145)\t1\n",
      "  (2, 393)\t1\n",
      "  (3, 477)\t1\n",
      "  (4, 210)\t1\n",
      "  (4, 323)\t1\n",
      "  (4, 468)\t1\n"
     ]
    }
   ],
   "source": [
    "# print(X_train.shape)\n",
    "\n",
    "cvec.fit(X_train)\n",
    "X_train_matrix = cvec.transform(X_train)\n",
    "print(X_train_matrix[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After three years, I finally _______ my daughter's cat.\n",
      "cat daughter\n"
     ]
    }
   ],
   "source": [
    "print(X_train.iloc[0,])\n",
    "print(cvec.get_feature_names()[64], cvec.get_feature_names()[94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_matrix = cvec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(max_depth=10, n_estimators=5)\n",
    "forest.fit(X_train_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7466493492138061"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_train_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7484203739522889"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_matrix = cvec.transform(X_test)\n",
    "forest.predict(X_test_matrix)\n",
    "forest.score(X_test_matrix, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7478401031592521"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_matrix = tvec.fit_transform(X_train)\n",
    "X_test_matrix = tvec.transform(X_test)\n",
    "\n",
    "forest.fit(X_train_matrix, y_train)\n",
    "forest.score(X_test_matrix, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4 result\n",
    "- The score between the sets are relatively small, the model seemed not to over/underfit. However, it stays as similar or same as baseline score. \n",
    "- Therefore, by using CVEC, TVEC, RF with `title` is not the best modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-tree-based Model 1 - Feature: `title` with CountVectorizer, TfidfVectorizer, Logistic Regression, KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X and y\n",
    "y = df['comments']\n",
    "X = df['title']\n",
    "\n",
    "X = X.apply(lambda x: PorterStemmer().stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"after three years, i finally _______ my daughter's cat.\",\n",
       "       'spoiler: this sub currently...', \"i'm gluten free and i'm sorri\",\n",
       "       ..., 'this crackhead mat',\n",
       "       'estonian president with klavan in kiev',\n",
       "       'hey kitty girls! it’s my birthday today &amp; my mum got me this card'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set CountVectorizer / HashingVectorizer / TFIDF\n",
    "# by removing english stop words, ngram_range(1,2)\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "hvec = HashingVectorizer(stop_words='english')\n",
    "tvec = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "\n",
    "lr = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7409413281753707"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `make_pipeline` - credit goes to Harsha\n",
    "pipe_lr = make_pipeline(cvec, lr)\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pipe_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7474532559638942"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline / fit / score for lr & hvec\n",
    "\n",
    "pipe_lr_2 = make_pipeline(hvec, lr)\n",
    "pipe_lr_2.fit(X_train, y_train)\n",
    "pipe_lr_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7460348162475822"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline / fit / score for lr & tvec\n",
    "\n",
    "pipe_lr_3 = make_pipeline(tvec, lr)\n",
    "pipe_lr_3.fit(X_train, y_train)\n",
    "pipe_lr_3.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7364925854287556"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline / fit / score for knn & cvec\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "pipe_knn = make_pipeline(cvec, knn)\n",
    "pipe_knn.fit(X_train, y_train)\n",
    "pipe_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7408768536428111"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_knn_2 = make_pipeline(tvec, knn)\n",
    "pipe_knn_2.fit(X_train, y_train)\n",
    "pipe_knn_2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 5 result\n",
    "- `title` with CVEC, TVEC with Logistic Regression, KNN doesn't show the best performance in my model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-tree-based Model 2 - Feature: `title`, `age`, `subreddit` with  TfidfVectorizer, Logistic Regression, KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the X and y\n",
    "\n",
    "y = df['comments']\n",
    "X = df[['title', 'age', 'subreddit']].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy subreddit\n",
    "X = pd.get_dummies(X, columns=['subreddit'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's tvec to title\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train_matrix = tvec.fit_transform(X_train['title'])\n",
    "X_test_matrix = tvec.transform(X_test['title'])\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_matrix.todense(),\n",
    "                         columns=tvec.get_feature_names(),\n",
    "                         index=X_train.index)\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test_matrix.todense(),\n",
    "                        columns=tvec.get_feature_names(),\n",
    "                        index=X_test.index)\n",
    "\n",
    "assert X_train.index.all() == X_train_df.index.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = pd.concat([X_train_df, X_train.drop('title', axis=1)], axis=1)\n",
    "X_test_all = pd.concat([X_test_df, X_test.drop('title', axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8368794326241135"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's fit& score with Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_all, y_train)\n",
    "logreg.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8300218310442977\n",
      "{'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# gridsearch for the params\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2']\n",
    "}\n",
    "gs = GridSearchCV(LogisticRegression(), param_grid=lr_params)\n",
    "gs.fit(X_train_all, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8368794326241135"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score on test set\n",
    "gs.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# let's check the best estimator\n",
    "print(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subreddit_AskReddit</th>\n",
       "      <td>85.757194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_cars</th>\n",
       "      <td>22.536326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_CFB</th>\n",
       "      <td>20.950063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_MemeEconomy</th>\n",
       "      <td>17.945319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit_Drama</th>\n",
       "      <td>17.717200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            coef\n",
       "subreddit_AskReddit    85.757194\n",
       "subreddit_cars         22.536326\n",
       "subreddit_CFB          20.950063\n",
       "subreddit_MemeEconomy  17.945319\n",
       "subreddit_Drama        17.717200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the best estimator coef\n",
    "\n",
    "coefs = pd.DataFrame(gs.best_estimator_.coef_[0], index=X_train_all.columns, columns=['coef'])\n",
    "coefs['coef'] = np.exp(coefs['coef'])\n",
    "coefs = coefs.sort_values(by='coef', ascending=False)\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.391433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.003595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         coef\n",
       "age  1.391433\n",
       "age  1.003595"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get odds ratio for the age\n",
    "coefs.loc['age', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^tried to find why I have two 'age' columns in my set. Will dig more.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647969052224372"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_all, y_train)\n",
    "knn.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 6 result\n",
    "- Among all the other model, it shows the best score `.83` in Logistic Regression. \n",
    "- Also, odds ratio can be interpreted as every minutes increase 39%  in odds having higher number of posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "\n",
    "How to get an attention in a sea of information, correction, in a sea of Reddit posts? Since the user-generated-site ranked the 6th most visited webpage in the world, it is more and more competitive to be remarkable. People spend a lot time in this categorical playground. In the Alexa Global Topsites 500 released the fact that 15 minutes and 8 seconds, in average, is the time people spend in the site daily. This figure outranked all of the following competitor in top 3 site from the same source: google(7:16), youtube(8:32), facebook(10:46). Therefore, hundreds of posts are judging, even in this very moment, instantly as either eye-catchy or nah-boring.\n",
    "\n",
    "  To analyze, therefore, the massive data is necessary whether which characteristics and/or features affect the most. As a data-scientist, I fetched fifty one thousands Reddit 'hot posts' in seven-days period and built the machine learning models to identify and find the odds. I utilized multiple data science methods, via python, such as CountVectorizer, TfidfVectorizer, RandomForest, Logistic Regression, etc. After evaluating multiple models, it shows that `title`, `age`, `subreddit` are the best features to build a model. \n",
    "\n",
    "  It appeared the most appeared word in the title for the popular posts were: \"new, one, first, now, time, day, will, made, game, today.\" Also, the most predictive popular subreddits that your can get attention are: r/AskReddit, r/cars, r/CFB, r/MemeEconomy, r/Drama. In conclusion, to make your post stays longer by capturing peoples attention, you will try to choose your subreddits and certain words in the title."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
